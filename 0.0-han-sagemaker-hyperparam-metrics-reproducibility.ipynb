{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "buried-heater",
   "metadata": {},
   "source": [
    "# A Quick Demonstration of SageMaker Reproducibility Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-enforcement",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Preparation](#preparation)\n",
    "3. [Download and prepare the data](#download)\n",
    "4. [Train + Hyperparameter tuning](#tune)\n",
    "5. [Deploy](#deploy)\n",
    "6. [Reproducing Objective Value](#wtf)\n",
    "7. [Teardown](#teardown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precise-christopher",
   "metadata": {},
   "source": [
    "# <a id='introduction'>Introduction</a>\n",
    "\n",
    "We encountered a problem where Sagemaker hyperparameter tuning produced a much higher `f1-score` than other hyperparameter sweep algorithms (`H2OAutoML`, `Scikit-Optimize Bayes`, `RandomizedSearchCV`) that we have tried and our own hand-tuned models. We got excited and decide to pull the model and verify if that `f1-score` displayed was real. Our manual verification using the best model and the same validation data produced an `f1-score` that is muchlower than what's displayed on Sagemaker Hyperparameter Tuning dashboard.\n",
    "\n",
    "This notebook demonstrates the misalignment between the displayed Objective Metrics Value on Hyperparameter Tuning dashboard vs the manually produced objective metrics value using the same model and same dataset.\n",
    "\n",
    "We will use SageMaker Python SDK, a high level SDK, to simplify the way we interact with SageMaker Hyperparameter Tuning.\n",
    "\n",
    "Example adopted from [HPO XGBoost Random Log Notebook](https://github.com/aws/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/xgboost_random_log/hpo_xgboost_random_log.ipynb), modified to support `SageMaker 2.25.2.`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-population",
   "metadata": {},
   "source": [
    "# <a id='preparation'>Preparation</a>\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "    The S3 bucket and prefix that you want to use for training and model data. This should be within the same region as SageMaker training.\n",
    "    The IAM role used to give training access to your data. See SageMaker documentation for how to create these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "automotive-pound",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np                                # For matrix operations and numerical processing\n",
    "import pandas as pd                               # For munging tabular data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "region = boto3.Session().region_name    \n",
    "smclient = boto3.Session().client('sagemaker')\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = sagemaker.Session().default_bucket()                     \n",
    "prefix = 'sagemaker/DEMO-hpo-xgboost-dm'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-healthcare",
   "metadata": {},
   "source": [
    "# <a id='download'>Download and prepare the data</a>\n",
    "\n",
    "Here we download the [direct marketing dataset](https://archive.ics.uci.edu/ml/datasets/bank+marketing) from UCI's ML Repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "noted-crisis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-05 01:20:20--  https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip\n",
      "Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n",
      "Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n",
      "HTTP request sent, awaiting response... 304 Not Modified\n",
      "File ‘bank-additional.zip’ not modified on server. Omitting download.\n",
      "\n",
      "Archive:  bank-additional.zip\n",
      "  inflating: bank-additional/.DS_Store  \n",
      "  inflating: __MACOSX/bank-additional/._.DS_Store  \n",
      "  inflating: bank-additional/.Rhistory  \n",
      "  inflating: bank-additional/bank-additional-full.csv  \n",
      "  inflating: bank-additional/bank-additional-names.txt  \n",
      "  inflating: bank-additional/bank-additional.csv  \n",
      "  inflating: __MACOSX/._bank-additional  \n"
     ]
    }
   ],
   "source": [
    "!wget -N https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank-additional.zip\n",
    "!unzip -o bank-additional.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-blood",
   "metadata": {},
   "source": [
    "Now let us load the data, apply some preprocessing, and upload the processed data to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "million-museum",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = pd.read_csv('./bank-additional/bank-additional-full.csv', sep=';')\n",
    "pd.set_option('display.max_columns', 500)     # Make sure we can see all of the columns\n",
    "pd.set_option('display.max_rows', 50)         # Keep the output on one page\n",
    "\n",
    "# Apply some feature processing\n",
    "data['no_previous_contact'] = np.where(data['pdays'] == 999, 1, 0)                                 # Indicator variable to capture when pdays takes a value of 999\n",
    "data['not_working'] = np.where(np.in1d(data['job'], ['student', 'retired', 'unemployed']), 1, 0)   # Indicator for individuals not actively employed\n",
    "model_data = pd.get_dummies(data)                                                                  # Convert categorical variables to sets of indicators\n",
    "\n",
    "# columns that should not be included in the input\n",
    "model_data = model_data.drop(['duration', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed'], axis=1)\n",
    "\n",
    "# split data\n",
    "train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), [int(0.7 * len(model_data)), int(0.9*len(model_data))])  \n",
    "\n",
    "# save preprocessed file to s3\n",
    "pd.concat([train_data['y_yes'], train_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('train.csv', index=False, header=False)\n",
    "pd.concat([validation_data['y_yes'], validation_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('validation.csv', index=False, header=False)\n",
    "pd.concat([test_data['y_yes'], test_data.drop(['y_no', 'y_yes'], axis=1)], axis=1).to_csv('test.csv', index=False, header=False)\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')).upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')).upload_file('validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-present",
   "metadata": {},
   "source": [
    "# <a id='tune'>Setup hyperparameter tuning</a>\n",
    "\n",
    "In this example, we are using SageMaker Python SDK to set up and manage the hyperparameter tuning job. We first configure the training jobs the hyperparameter tuning job will launch by initiating an estimator, and define the static hyperparameter and objective.\n",
    "\n",
    "The objective metrics we use is `validation:auc`, which we presume to be `auc` for `validation` data set.  We will only train for one iteration for this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "interracial-association",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deprecated in sagemaker v2.\n",
    "# s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "# s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')\n",
    "\n",
    "s3_input_train = TrainingInput(s3_data='s3://{}/{}/train/train.csv'.format(bucket, prefix), content_type='text/csv')\n",
    "s3_input_validation = TrainingInput(s3_data='s3://{}/{}/validation/validation.csv'.format(bucket, prefix), content_type='text/csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "smaller-danger",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "container = sagemaker.image_uris.retrieve(\n",
    "    framework='xgboost',\n",
    "    region=boto3.Session().region_name,\n",
    "    version='1.2-1',\n",
    "    py_version='py3')\n",
    "\n",
    "xgb = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role, \n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.4xlarge',\n",
    "    output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "xgb.set_hyperparameters(\n",
    "    eval_metric='auc',\n",
    "    objective='binary:logistic',\n",
    "    num_round=100,\n",
    "    rate_drop=0.3,\n",
    "    tweedie_variance_power=1.4\n",
    ")\n",
    "\n",
    "objective_metric_name = 'validation:auc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "armed-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    'alpha': ContinuousParameter(0.01, 10, scaling_type=\"Logarithmic\"),\n",
    "    'lambda': ContinuousParameter(0.01, 10, scaling_type=\"Logarithmic\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "entertaining-harrison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................................!\n"
     ]
    }
   ],
   "source": [
    "tuner_log = HyperparameterTuner(\n",
    "    xgb,\n",
    "    objective_metric_name,\n",
    "    hyperparameter_ranges,\n",
    "    max_jobs=1,\n",
    "    max_parallel_jobs=1,\n",
    "    strategy='Random'\n",
    ")\n",
    "\n",
    "tuner_log.fit({'train': s3_input_train, 'validation': s3_input_validation}, include_cls_metadata=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-scout",
   "metadata": {},
   "source": [
    "Check if jobs have finished and get the logs of the job.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "conservative-brook",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Completed'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boto3.client('sagemaker').describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner_log.latest_tuning_job.job_name)['HyperParameterTuningJobStatus']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "contemporary-cleaner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha</th>\n",
       "      <th>lambda</th>\n",
       "      <th>TrainingJobName</th>\n",
       "      <th>TrainingJobStatus</th>\n",
       "      <th>FinalObjectiveValue</th>\n",
       "      <th>TrainingStartTime</th>\n",
       "      <th>TrainingEndTime</th>\n",
       "      <th>TrainingElapsedTimeSeconds</th>\n",
       "      <th>scaling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.59165</td>\n",
       "      <td>0.240368</td>\n",
       "      <td>sagemaker-xgboost-210305-0120-001-1d95d186</td>\n",
       "      <td>Completed</td>\n",
       "      <td>0.75957</td>\n",
       "      <td>2021-03-05 01:22:24+00:00</td>\n",
       "      <td>2021-03-05 01:23:28+00:00</td>\n",
       "      <td>64.0</td>\n",
       "      <td>log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     alpha    lambda                             TrainingJobName  \\\n",
       "0  0.59165  0.240368  sagemaker-xgboost-210305-0120-001-1d95d186   \n",
       "\n",
       "  TrainingJobStatus  FinalObjectiveValue         TrainingStartTime  \\\n",
       "0         Completed              0.75957 2021-03-05 01:22:24+00:00   \n",
       "\n",
       "            TrainingEndTime  TrainingElapsedTimeSeconds scaling  \n",
       "0 2021-03-05 01:23:28+00:00                        64.0     log  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_log = boto3.client('sagemaker').describe_hyper_parameter_tuning_job(\n",
    "    HyperParameterTuningJobName=tuner_log.latest_tuning_job.job_name)['HyperParameterTuningJobStatus']\n",
    "\n",
    "assert status_log == 'Completed', \"First must be completed, was {}\".format(status_log)\n",
    "\n",
    "df_log = sagemaker.HyperparameterTuningJobAnalytics(tuner_log.latest_tuning_job.job_name).dataframe()\n",
    "df_log['scaling'] = 'log'\n",
    "final_objective_value = df_log['FinalObjectiveValue'][0]\n",
    "df_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-receiver",
   "metadata": {},
   "source": [
    "# <a id='deploy'>Deploy</a>\n",
    "\n",
    "After the model's trained, we will deploy it using Sagemaker to keep AWS happy and our wallets empty.\n",
    "\n",
    "[Reference deployment code is here.](https://aws.amazon.com/getting-started/hands-on/build-train-deploy-machine-learning-model-sagemaker/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bibliographic-hayes",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-03-05 01:23:28 Starting - Preparing the instances for training\n",
      "2021-03-05 01:23:28 Downloading - Downloading input data\n",
      "2021-03-05 01:23:28 Training - Training image download completed. Training in progress.\n",
      "2021-03-05 01:23:28 Uploading - Uploading generated training model\n",
      "2021-03-05 01:23:28 Completed - Training job completed\n",
      "-------------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor = tuner_log.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greenhouse-bottom",
   "metadata": {},
   "source": [
    "# <a id='wtf'> Reproducing Objective Value</a>\n",
    "\n",
    "For this tuning job, we set our `objective_metric_name` as `validation:auc` and the tuner found the `FinalObjectiveValue` of `0.75241`.\n",
    "\n",
    "Awesomesauce. Let's try to reproduce the same `FinalObjectiveValue` by running our validation data through the trained model and [calculate the auc](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html) using scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "global-bibliography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reproduced validation:auc is 0.5949. Advertised FinalObjectiveValue is 0.7596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validation_data_array = validation_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array\n",
    "xgb_predictor.serializer = CSVSerializer() # set the serializer type\n",
    "predictions = xgb_predictor.predict(validation_data_array).decode('utf-8') # predict!\n",
    "predictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an array\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(validation_data['y_yes'], np.round(predictions_array))\n",
    "validation_auc = metrics.auc(fpr, tpr)\n",
    "print(f'Reproduced validation:auc is {validation_auc:.4}. Advertised FinalObjectiveValue is {final_objective_value:.4}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-technology",
   "metadata": {},
   "source": [
    "Hmm, what's going on?  Maybe the FinalObjectiveValue is auc calculated on the training set? Let's try that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "danish-blair",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reproduced train:auc is 0.6946. Advertised FinalObjectiveValue is 0.7596\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_array = train_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array\n",
    "xgb_predictor.serializer = CSVSerializer() # set the serializer type\n",
    "predictions = xgb_predictor.predict(train_data_array).decode('utf-8') # predict!\n",
    "predictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an array\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(train_data['y_yes'], np.round(predictions_array))\n",
    "train_auc = metrics.auc(fpr, tpr)\n",
    "print(f'Reproduced train:auc is {train_auc:.4}. Advertised FinalObjectiveValue is {final_objective_value:.4}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "historical-while",
   "metadata": {},
   "source": [
    "Okay. So, `FinalObjectiveValue` is neither `validation:auc` or `train:auc`. So, what does that number represent? On Sagemaker Hyperparameter Tuner, it does say that the number is `validation:auc`. What is it then?\n",
    "\n",
    "This confirms our original finding of Sagemaker Hyperparameter Tuner producing a too-good-to-be-true `f1-score` for our data.\n",
    "\n",
    "We could download the model and run inference locally to verify it again, but are too lazy to code that out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "impaired-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_name = df_log['TrainingJobName'][0]\n",
    "model_file = f's3://{bucket}/{prefix}/output/{training_job_name}/output/model.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-listening",
   "metadata": {},
   "source": [
    "# <a id='teardown'>Teardown</a>\n",
    "\n",
    "Teardown the deployed endpoint and the model so we can afford food tomorrow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "incident-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.delete_endpoint()\n",
    "xgb_predictor.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Custom (ef-datascience)",
   "language": "python",
   "name": "ef-datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
